services:
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: unless-stopped
    runtime: nvidia

    ports:
      - "${VLLM_PORT:-8000}:8000"

    environment:
      - HF_HOME=/models
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    volumes:
      - ../vllm/current_model:/models:ro

    command: >-
      --model /models
      --quantization awq
      --max-model-len ${MODEL_MAXLEN:-8192}
      --served-model-name mistral7b-awq
      --trust-remote-code
      --host 0.0.0.0

    networks: [echo]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      - vllm-server

    ports:
      - "${OPENWEBUI_PORT:-8080}:8080"

    environment:
      - OPENAI_API_BASE_URL=http://vllm-server:8000/v1
      - WEBUI_AUTH=${WEBUI_AUTH:-false}

    volumes:
      - ../openwebui/data:/app/backend/data

    networks: [echo]

networks:
  echo:
    driver: bridge