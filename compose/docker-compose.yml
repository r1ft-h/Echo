services:
  vllm-server:
    image: vllm/vllm-openai:0.2.4
    container_name: vllm-server
    restart: unless-stopped
    runtime: nvidia

    ports:
      - "${VLLM_PORT:-8000}:8000"

    environment:
      - HF_HOME=/models

    volumes:
      - ../vllm/current_model:/models:ro

    command: >-
      --model /models
      --max-model-len 2048
      --served-model-name mistral7b-awq
      --host 0.0.0.0

    networks: [shai]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      - vllm-server

    ports:
      - "${OPENWEBUI_PORT:-8080}:8080"

    environment:
      - OPENAI_API_BASE_URL=http://vllm-server:8000/v1
      - WEBUI_AUTH=${WEBUI_AUTH:-false}

    volumes:
      - ../openwebui/data:/app/backend/data

    networks: [shai]

networks:
  shai:
    driver: bridge
